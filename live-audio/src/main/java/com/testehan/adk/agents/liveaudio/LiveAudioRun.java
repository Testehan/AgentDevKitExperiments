package com.testehan.adk.agents.liveaudio;

import com.google.adk.agents.LiveRequestQueue;
import com.google.adk.agents.RunConfig;
import com.google.adk.events.Event;
import com.google.adk.runner.Runner;
import com.google.adk.sessions.InMemorySessionService;
import com.google.common.collect.ImmutableList;
import com.google.genai.types.*;
import io.reactivex.rxjava3.core.Flowable;

import javax.sound.sampled.*;
import java.util.UUID;
import java.util.concurrent.*;
import java.util.concurrent.atomic.AtomicBoolean;

import com.testehan.adk.agents.streaming.ScienceTeacherAgent;

/** Main class to demonstrate running the {@link LiveAudioAgent} for a voice conversation. */
public final class LiveAudioRun {
    private final String userId;
    private final String sessionId;
    private final Runner runner;

    private static final javax.sound.sampled.AudioFormat MIC_AUDIO_FORMAT =
            new javax.sound.sampled.AudioFormat(16000.0f, 16, 1, true, false);

    private static final javax.sound.sampled.AudioFormat SPEAKER_AUDIO_FORMAT =
            new javax.sound.sampled.AudioFormat(24000.0f, 16, 1, true, false);

    private static final int BUFFER_SIZE = 4096;

    public LiveAudioRun() {
        this.userId = "test_user";
        var appName = "LiveAudioApp";
        this.sessionId = UUID.randomUUID().toString();

        InMemorySessionService sessionService = new InMemorySessionService();
        this.runner = new Runner(ScienceTeacherAgent.ROOT_AGENT, appName, null, sessionService);

        ConcurrentMap<String, Object> initialState = new ConcurrentHashMap<>();
        var unused = sessionService.createSession(appName, userId, initialState, sessionId).blockingGet();
    }

    private void runConversation() throws Exception {
        System.out.println("Initializing microphone input and speaker output...");

        RunConfig runConfig =
                RunConfig.builder()
                        .setStreamingMode(RunConfig.StreamingMode.BIDI)
                        .setResponseModalities(ImmutableList.of(new Modality("AUDIO")))
                        .setSpeechConfig(
                                SpeechConfig.builder()
                                        .voiceConfig(
                                                VoiceConfig.builder()
                                                        .prebuiltVoiceConfig(
                                                                PrebuiltVoiceConfig.builder().voiceName("Aoede").build())
                                                        .build())
                                        .languageCode("en-US")
                                        .build())
                        .build();

        LiveRequestQueue liveRequestQueue = new LiveRequestQueue();

        Flowable<Event> eventStream =
                this.runner.runLive(
                        runner.sessionService().createSession(userId, sessionId).blockingGet(),
                        liveRequestQueue,
                        runConfig);

        AtomicBoolean isRunning = new AtomicBoolean(true);
        AtomicBoolean conversationEnded = new AtomicBoolean(false);
        ExecutorService executorService = Executors.newFixedThreadPool(2);

        // Task for capturing microphone input
        Future<?> microphoneTask = executorService.submit(() -> captureAndSendMicrophoneAudio(liveRequestQueue, isRunning));

        // Task for processing agent responses and playing audio
        Future<?> outputTask = executorService.submit(
                        () -> { // TODO This could be simplified ...as in move the try catch inside the processAudioOutput method..
                            try {
                                processAudioOutput(eventStream, isRunning, conversationEnded);
                            } catch (Exception e) {
                                System.err.println("Error processing audio output: " + e.getMessage());
                                e.printStackTrace();
                                isRunning.set(false);
                            }
                        });

        // Wait for user to press Enter to stop the conversation
        System.out.println("Conversation started. Press Enter to stop...");
        System.in.read();

        System.out.println("Ending conversation...");
        isRunning.set(false);

        try {
            // Give some time for ongoing processing to complete
            microphoneTask.get(2, TimeUnit.SECONDS);
            outputTask.get(2, TimeUnit.SECONDS);
        } catch (Exception e) {
            System.out.println("Stopping tasks...");
        }

        liveRequestQueue.close();
        executorService.shutdownNow();
        System.out.println("Conversation ended.");
    }

    private void captureAndSendMicrophoneAudio(LiveRequestQueue liveRequestQueue, AtomicBoolean isRunning) {
        TargetDataLine micLine = null;
        try {
            DataLine.Info info = new DataLine.Info(TargetDataLine.class, MIC_AUDIO_FORMAT);
            if (!AudioSystem.isLineSupported(info)) {
                System.err.println("Microphone line not supported!");
                return;
            }

            micLine = (TargetDataLine) AudioSystem.getLine(info);
            micLine.open(MIC_AUDIO_FORMAT);
            micLine.start();

            System.out.println("Microphone initialized. Start speaking...");

            byte[] buffer = new byte[BUFFER_SIZE];
            int bytesRead;

            while (isRunning.get()) {
                bytesRead = micLine.read(buffer, 0, buffer.length);

                if (bytesRead > 0) {
                    byte[] audioChunk = new byte[bytesRead];
                    System.arraycopy(buffer, 0, audioChunk, 0, bytesRead);

                    Blob audioBlob = Blob.builder().data(audioChunk).mimeType("audio/pcm").build();

                    liveRequestQueue.realtime(audioBlob);
                }
            }
        } catch (LineUnavailableException e) {
            System.err.println("Error accessing microphone: " + e.getMessage());
            e.printStackTrace();
        } finally {
            if (micLine != null) {
                micLine.stop();
                micLine.close();
            }
        }
    }

    private void processAudioOutput(Flowable<Event> eventStream, AtomicBoolean isRunning, AtomicBoolean conversationEnded) {
        SourceDataLine speakerLine = null;
        try {
            DataLine.Info info = new DataLine.Info(SourceDataLine.class, SPEAKER_AUDIO_FORMAT);
            if (!AudioSystem.isLineSupported(info)) {
                System.err.println("Speaker line not supported!");
                return;
            }

            final SourceDataLine finalSpeakerLine = (SourceDataLine) AudioSystem.getLine(info);
            finalSpeakerLine.open(SPEAKER_AUDIO_FORMAT);
            finalSpeakerLine.start();

            System.out.println("Speaker initialized.");

            for (Event event : eventStream.blockingIterable()) {
                if (!isRunning.get()) {
                    break;
                }

                // TODO the next 2 lines should be added to google docs example, as they are missing and the method process event is never called.
                // you should do this on 6th of july ?
                AtomicBoolean audioReceived = new AtomicBoolean(false);
                processEvent(event, audioReceived);  // Call it here

                event.content().ifPresent(content -> content.parts().ifPresent(parts -> parts.forEach(part -> playAudioData(part, finalSpeakerLine))));
            }

            speakerLine = finalSpeakerLine; // Assign to outer variable for cleanup in finally block
        } catch (LineUnavailableException e) {
            System.err.println("Error accessing speaker: " + e.getMessage());
            e.printStackTrace();
        } finally {
            if (speakerLine != null) {
                speakerLine.drain();
                speakerLine.stop();
                speakerLine.close();
            }
            conversationEnded.set(true);
        }
    }

    private void playAudioData(Part part, SourceDataLine speakerLine) {
        part.inlineData()
                .ifPresent(
                        inlineBlob ->
                                inlineBlob
                                        .data()
                                        .ifPresent(
                                                audioBytes -> {
                                                    if (audioBytes.length > 0) {
                                                        System.out.printf(
                                                                "Playing audio (%s): %d bytes%n",
                                                                inlineBlob.mimeType(),
                                                                audioBytes.length);
                                                        speakerLine.write(audioBytes, 0, audioBytes.length);
                                                    }
                                                }));
    }


    private void processEvent(Event event, java.util.concurrent.atomic.AtomicBoolean audioReceived) {
        event
                .content()
                .ifPresent(
                        content ->
                                content
                                        .parts()
                                        .ifPresent(parts -> parts.forEach(part -> logReceivedAudioData(part, audioReceived))));
    }




    private void logReceivedAudioData(Part part, AtomicBoolean audioReceived) {
        part.inlineData()
                .ifPresent(
                        inlineBlob ->
                                inlineBlob
                                        .data()
                                        .ifPresent(
                                                audioBytes -> {
                                                    if (audioBytes.length > 0) {
                                                        System.out.printf(
                                                                "    Audio (%s): received %d bytes.%n",
                                                                inlineBlob.mimeType(),
                                                                audioBytes.length);
                                                        audioReceived.set(true);
                                                    } else {
                                                        System.out.printf(
                                                                "    Audio (%s): received empty audio data.%n",
                                                                inlineBlob.mimeType());
                                                    }
                                                }));
    }

    public static void main(String[] args) throws Exception {
        LiveAudioRun liveAudioRun = new LiveAudioRun();
        liveAudioRun.runConversation();
        System.out.println("Exiting Live Audio Run.");
    }
}
